{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nimport matplotlib.pyplot as plt\n\nimport tqdm as tqdm\n\nimport PIL\nfrom PIL import Image\n\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -xzf /kaggle/input/lfwpeople/lfw-funneled.tgz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_triplets(data_dir):\n    people_pathes = [os.path.join(data_dir, person) for person in os.listdir(data_dir) if '.txt' not in person]\n    people_count = len(people_pathes)\n\n    singles = []\n    trimmed = []\n    \n    for person in people_pathes:\n        person_images_pathes = [os.path.join(person, image_path) for image_path in os.listdir(person)]\n        if len(person_images_pathes) == 1:\n            singles.append(os.path.join(person, os.listdir(person)[0]))\n        else:\n            trimmed.append(person)\n        \n    print('This dataset contain {}, {} of which has a single, and {} have two or more '\n          .format(people_count, len(singles), len(trimmed)))\n    \n    anchors = []\n    positives = []\n    negtives = []\n    \n    trimmed_count = len(trimmed)\n\n    for person in trimmed:\n        person_images_pathes = [os.path.join(person, image_path) for image_path in os.listdir(person)]\n        \n        for image in person_images_pathes:\n            anchor = image\n            positive = person_images_pathes[random.randrange(0, len(person_images_pathes) - 1)]\n            negtive = singles[random.randrange(0, len(singles) - 1)]\n            \n            anchors.append(anchor)\n            positives.append(positive)\n            negtives.append(negtive)\n    \n    return anchors, positives, negtives\n\ndataset_dir = '/kaggle/working/lfw_funneled'\nANCH, POS, NEG = generate_triplets(dataset_dir)\n\nTRIPLETS_LEN = len(ANCH)\ndef get_triplets_by_index(index):\n    return ANCH[index], POS[index], NEG[index]\n\nif len(ANCH) == len(POS) and len(NEG) == len(NEG):\n    print('Generated {} triplets'.format(len(NEG)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(get_triplets_by_index(TRIPLETS_LEN - 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_INPUT_SIZE = (224, 224)\n\ntransform = transforms.Compose(\n    [transforms.Resize(IMAGE_INPUT_SIZE),\n    transforms.ToTensor()]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_and_transform_triplets_by_index(index):\n    a, p, n = get_triplets_by_index(index)\n    \n    a_tensor = transform(Image.open(a))\n    p_tensor = transform(Image.open(p))\n    n_tensor = transform(Image.open(n))\n    \n    return a_tensor, p_tensor, n_tensor\n\ndef load_and_transform_batch(start, end):\n    try:\n        anchor_batch   = torch.stack([ transform(Image.open(ANCH[i]))  for i in range(start, end) ]).to(device)\n        positive_batch = torch.stack([ transform(Image.open(POS[i]))   for i in range(start, end) ]).to(device)                                \n        negtive_batch  = torch.stack([ transform(Image.open(NEG[i]))   for i in range(start, end) ]).to(device)       \n        return anchor_batch, positive_batch, negtive_batch\n    except:\n        pass\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_triplets_by_index(index):\n    a_tensor, p_tensor, n_tensor = load_and_transform_triplets_by_index(index)\n    \n    fig=plt.figure(figsize=(32, 8))\n\n    fig.add_subplot(1, 3, 1)\n    plt.imshow(a_tensor.view(224, 224, 3))\n    plt.text(0.02, 0.5,'Anchor', fontsize=18)\n\n    fig.add_subplot(1, 3, 2)\n    plt.imshow(p_tensor.view(224, 224, 3))\n    plt.text(0.02, 0.5,'Positive', fontsize=18)\n\n    fig.add_subplot(1, 3, 3)\n    plt.imshow(n_tensor.view(224, 224, 3))\n    plt.text(0.02, 0.5,'Negitive', fontsize=18)\n\n    plt.show()\n        \nplot_triplets_by_index(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the model\nmodel = models.resnet18(pretrained=False).to(device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.1e-3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    running_loss = 0.0\n    \n    for i in range( int(len(ANCH)) ):\n        try:\n            A, P, N = load_and_transform_batch(i, i + 50)   \n            \n            optimizer.zero_grad()\n            \n            anchor_out = model(A)\n            negitive_out = model(P)        \n            positive_out = model(N)\n            \n            loss = triplet_loss(anchor_out, negitive_out, positive_out)\n            loss.backward()\n            optimizer.step()\n            \n            # print statistics\n            running_loss += loss.item()\n            if i % 10 == 9:    # print every 200 mini-batches\n                print('[%d, %5d] loss: %.9f' %\n                      (epoch + 1, i + 1, running_loss / 2000))\n                running_loss = 0.0\n\n        except:\n            print(\"epoch {} done\".format(epoch))\n            break\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'facenet.ptm')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check(a, b):\n    a_tensor = transform(Image.open(a))\n    p_tensor = transform(Image.open(b))\n    \n    return torch.dist(model(a_tensor.view(1, 3, 224, 224).to(device))\n                      , model(p_tensor.view(1, 3, 224, 224).to(device))\n                     )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare(index):\n    print(check(ANCH[index], POS[index]), 'anchor - pos')\n    print(check(ANCH[index], NEG[index]), 'anchor - neg')\n    \nfor sample in range(int(len(ANCH)* 0.9)):\n    compare(sample)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}